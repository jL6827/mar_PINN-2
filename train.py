#åšäº†å¤šæ¬¡ä¿®æ­£ï¼š10.25ä¿®æ­£çš„æ˜¯ï¼šåœ¨epoch=7kæ—¶ï¼ŒRoä¼šå¿«é€Ÿå¢é•¿ï¼›è§£å†³å§åŠæ³•ï¼šå¯¹Roç”¨æ›´å°çš„å­¦ä¹ ç‡ï¼›
#åŠ¨æ€è°ƒæ•´Roçš„æƒé‡
#è¿™éƒ¨åˆ†ä¸å¥½å°†å®ƒåˆ†æˆè®­ç»ƒå’Œé¢„æµ‹çš„åŸå› æ˜¯ï¼šæ•°æ®é›†åˆ†å‰²æ˜¯éšæœºçš„ï¼Œæ‰€ä»¥å•ç‹¬åˆ†å¼€æ¯”è¾ƒéº»çƒ¦

import argparse
import os
import torch
from losses import direction_loss, value_loss, activation_loss
import pandas as pd
import torch.nn.functional as F
from config import ModelConfig
from data_loader import split_dataset_random, load_csv_data_from_df
from physics_model import EnhancedPhysicsInformedThermocline
from compute_approximate_velocity import compute_velocity
from compute_physics_residual import compute_residuals
from physics_residual import geometric_constraint
from utils import get_device, prepare_inputs, LossManager
from save_utils import PredictionSaver

import argparse

def parse_args_for_train():
    p = argparse.ArgumentParser()
    # é»˜è®¤ä½¿ç”¨é¡¹ç›®å†… data/ å’Œ outputs/ å­ç›®å½•ï¼ˆç›¸å¯¹è·¯å¾„ï¼‰
    p.add_argument("--input", "-i", default="data/processed_data_mean_train.csv",
                   help="è¾“å…¥ CSV æ–‡ä»¶è·¯å¾„ï¼ˆç›¸å¯¹é¡¹ç›®æ ¹ç›®å½•ï¼‰ï¼Œé»˜è®¤: data/processed_data_mean_train.csv")
    p.add_argument("--output", "-o", default="outputs/run1",
                   help="è®­ç»ƒè¾“å‡ºç›®å½•ï¼ˆç›¸å¯¹é¡¹ç›®æ ¹ç›®å½•ï¼‰ï¼Œé»˜è®¤: outputs/run1")
    p.add_argument("--device", default=None, help="å¯é€‰ï¼šcuda:0 æˆ– cpu")
    return p.parse_args()


def train_prediction_model(input_path, output_dir, device=None):  # ä¿®å¤ï¼šå°† - æ”¹ä¸º _
    os.makedirs(output_dir, exist_ok=True)

    # è®¾å¤‡å¤„ç†ï¼šå¦‚æœç”¨æˆ·é€šè¿‡ CLI ä¼ å…¥ deviceï¼Œåˆ™ä½¿ç”¨è¯¥è®¾å¤‡å­—ç¬¦ä¸²ï¼Œå¦åˆ™ä½¿ç”¨é¡¹ç›®è‡ªå¸¦çš„ get_device()
    if device is not None:
        device = torch.device(device)
    else:
        device = get_device()

    train_df, test_df = split_dataset_random(input_path)
    t, x, y, z, u_true, v_true, scaler_mgr, original_df = load_csv_data_from_df(train_df, device)

    config = ModelConfig(
        Ro=0.024,
        omega_0=30.0,
        use_scaler=True,
        scaler_mgr=scaler_mgr,
        depth_scaler=scaler_mgr.depth_scaler,
        velocity_scale=1.0,
        grad_clip=10.0
    )
    model = EnhancedPhysicsInformedThermocline(config).to(device)

    # ==================== åˆå§‹åŒ– ====================
    base_lr = 1e-4
    ro_base_lr = 5e-4
    #initial_ro_weight = 0.5

    # å‚æ•°åˆ†ç¦»
    ro_params = [p for n, p in model.named_parameters() if n == 'Ro']
    other_params = [p for n, p in model.named_parameters() if n != 'Ro']

    optimizer = torch.optim.Adam([
        {'params': other_params, 'lr': base_lr},
        {'params': ro_params, 'lr': ro_base_lr}
    ])

    #check_trainable_parameters(model)

    # æŸå¤±æƒé‡ç®¡ç†
    loss_manager = LossManager({
        'data': 3.5, 'dir': 5, 'phys': 3.0,
        'cont': 1.0, 'geo': 0.2, 'act': 5
    })

    # ==================== æ ¸å¿ƒè¾…åŠ©å‡½æ•° ====================
    def get_dynamic_weights(epoch):
        """åŠ¨æ€æƒé‡è°ƒåº¦ï¼ˆåˆ†æ®µç­–ç•¥ï¼‰"""
        if epoch < 4000:
            ro_w = 0.5 * (1 - 0.5 * epoch / 4000)
        # é˜¶æ®µ2ï¼š4000-7000ä¸ªepochï¼Œä»0.25è¡°å‡åˆ°0.1
        elif epoch < 7000:
            ro_w = 0.25 * (1 - 0.6 * (epoch - 4000) / 3000)
        else:
            ro_w = 0.1

        return ro_w, 0.02

    def update_ro_lr(epoch):
        """æ›´æ–°Roå­¦ä¹ ç‡"""
        initial_ro_weight = 0.5  # ç¬¬ä¸€é˜¶æ®µçš„åˆå§‹å€¼
        ro_weight, _ = get_dynamic_weights(epoch)
        lr_scale = max(0.1, ro_weight / initial_ro_weight)

        for pg in optimizer.param_groups:
            if pg['params'][0] is model.Ro:
                pg['lr'] = ro_base_lr * lr_scale
                break

        return lr_scale

    def compute_ro_regularization(model, ro_weight, b_weight):
        """RoåŠBæ ‡é‡æ­£åˆ™é¡¹"""
        ro_min, ro_max = 0.02, 0.1
        ro_reg = ro_weight * (
                F.softplus(50 * (ro_min - model.Ro)) +
                F.softplus(50 * (model.Ro - ro_max))
        )

        b0_reg = b_weight * F.softplus(20 * (model.B0_scalar - 0.05))
        b1_reg = b_weight * F.softplus(20 * (model.B1_scalar - 0.05))

        total_reg = ro_reg + b0_reg + b1_reg
        return total_reg, ro_reg, b0_reg, b1_reg

    # ==================== è®­ç»ƒå¾ªç¯ ====================
    ro_history = []
    num_epochs = 8000

    for epoch in range(num_epochs):
        # åŠ¨æ€å­¦ä¹ ç‡å’Œæƒé‡
        ro_lr_scale = update_ro_lr(epoch)  # âœ… è·å–ç¼©æ”¾å› å­
        ro_weight, b_weight = get_dynamic_weights(epoch)

        # åŠ¨æ€è°ƒæ•´ç‰©ç†æƒé‡ï¼ˆä»…åœ¨éœ€è¦æ—¶ï¼‰
        if epoch >= 3000 and epoch % 500 == 0:
            step = (epoch - 3000) // 500
            loss_manager.set_weights({
                'phys': min(5.0, 3.0 + 0.2 * step),
                'cont': min(2.0, 1.0 + 0.1 * step),
                'geo': min(0.5, 0.2 + 0.05 * step),
                'dir': min(8.0, 5.0 + 0.2 * step)
            })

        optimizer.zero_grad()
        x, y, z, t = prepare_inputs(x, y, z, t)

        # å‰å‘ä¼ æ’­
        (u_pred, v_pred, u_bar, v_bar, theta, eta, Z0,
         P, g1, g2, h1, h2, time_phase_C2, time_phase_C3) = compute_velocity(model, x, y, z, t)

        # è®¡ç®—æ‰€æœ‰æŸå¤±
        losses = {
            'data': value_loss(u_pred, v_pred, u_true, v_true),
            'dir': direction_loss(u_pred, v_pred, u_true, v_true),
            'act': activation_loss(theta, eta),
            'geo': geometric_constraint(g1, g2, Z0, x, y),
        }

        # ç‰©ç†çº¦æŸæŸå¤±
        res_u, res_v, res_cont = compute_residuals(model, x, y, z, t)
        losses['phys'] = torch.mean(res_u ** 2 + res_v ** 2)
        losses['cont'] = torch.mean(res_cont ** 2)

        # Z0æ­£åˆ™åŒ–
        grad_z0 = [torch.autograd.grad(Z0, v, grad_outputs=torch.ones_like(Z0),
                                       create_graph=True)[0] for v in [x, y]]
        z0_reg = 0.5 * torch.mean(Z0) ** 2 + 0.2 * (torch.var(Z0) - 0.1) ** 2 + \
                 0.1 * torch.mean(sum(g ** 2 for g in grad_z0))

        # âœ… è®¡ç®—Roæ­£åˆ™åŒ–ï¼ˆç°åœ¨è¿”å›å…ƒç»„ï¼‰
        reg_total, ro_reg, b0_reg, b1_reg = compute_ro_regularization(model, ro_weight, b_weight)

        # æ€»æŸå¤±
        total_loss = loss_manager.compute_total_loss(losses, epoch=epoch)
        total_loss += reg_total + z0_reg

        # Roå˜åŒ–ç‡çº¦æŸï¼ˆåæœŸç¨³å®šï¼‰
        ro_change_penalty = torch.tensor(0.0, device=device)  # âœ… åˆå§‹åŒ–
        if epoch > 7000 and ro_history:
            ro_change_penalty = 0.1 * (model.Ro - ro_history[-1]) ** 2
            total_loss += ro_change_penalty

        total_loss.backward()

        # â­ ç®€åŒ–æ¢¯åº¦æ§åˆ¶ï¼šåªä¿ç•™å…³é”®éƒ¨åˆ†
        if model.Ro.grad is not None and torch.abs(model.Ro.grad) > 1.0:
            model.Ro.grad.data.clamp_(-0.5, 0.5)

        optimizer.step()
        with torch.no_grad():
            model.Ro.clamp_(1e-2, 0.5)

        if torch.isnan(total_loss):
            print(f"[Epoch {epoch}] NaN detected, stopping.")
            break

        ro_history.append(model.Ro.item())

        # ğŸ¯ å®Œæ•´ç›‘æ§ï¼ˆæ‰€æœ‰å˜é‡éƒ½è¢«æ­£ç¡®å®šä¹‰ï¼‰
        if epoch % 1000 == 0:
            # è·å–å½“å‰å­¦ä¹ ç‡
            current_lrs = [f"Ro: {pg['lr']:.2e}" if pg['params'][0] is model.Ro
                           else f"Other: {pg['lr']:.2e}" for pg in optimizer.param_groups]

            print(f"{'=' * 70}")
            print(f"[Epoch {epoch}] çŠ¶æ€ç›‘æ§")
            print(f"{'=' * 70}")

            # å­¦ä¹ ç‡ä¿¡æ¯
            print(f"  å­¦ä¹ ç‡: {', '.join(current_lrs)}")
            print(f"  Roæ­£åˆ™æƒé‡: {ro_weight:.4f}, Roå­¦ä¹ ç‡ç¼©æ”¾: {ro_lr_scale:.4f}")

            # ç½‘ç»œå‚æ•°
            print(f"  Î¸ mean: {theta.mean().item():.6f}, Î· mean: {eta.mean().item():.6f}")
            print(f"  u_pred mean: {u_pred.mean().item():.6f}, v_pred mean: {v_pred.mean().item():.6f}")
            print(f"  Zâ‚€ mean: {Z0.mean().item():.6f}, Zâ‚€ var: {Z0.var().item():.6f}")

            # Roå‚æ•°å’Œæ­£åˆ™åŒ–
            print(f"  Ro value: {model.Ro.item():.6f}")
            print(f"  Ro regularization: {ro_reg.item():.6f}")

            # Bæ ‡é‡å‚æ•°å’Œæ­£åˆ™åŒ–
            print(f"  B0_scalar: {model.B0_scalar.item():.6f}, B0_reg: {b0_reg.item():.6f}")
            print(f"  B1_scalar: {model.B1_scalar.item():.6f}, B1_reg: {b1_reg.item():.6f}")

            # Roå˜åŒ–æƒ©ç½šé¡¹ï¼ˆåæœŸï¼‰
            if epoch > 7000:
                print(f"  Ro change penalty: {ro_change_penalty.item():.6f}")

            # å„é¡¹æŸå¤±
            print(f"æŸå¤±åˆ†é‡: ")
            print(f"    Data: {losses['data'].item():.6f}")
            print(f"    Dir:  {losses['dir'].item():.6f}")
            print(f"    Phys: {losses['phys'].item():.6f}")
            print(f"    Cont: {losses['cont'].item():.6f}")
            print(f"    Geo:  {losses['geo'].item():.6f}")
            print(f"    Act:  {losses['act'].item():.6f}")
            print(f"    Z0_reg: {z0_reg.item():.6f}")
            print(f"    Ro_reg: {ro_reg.item():.6f}")
            print(f"  Total Loss: {total_loss.item():.6f}")

            # æ¢¯åº¦ä¿¡æ¯
            if model.Ro.grad is not None:
                print(f" Ro grad: {model.Ro.grad.item(): .6f} ")

            print(f"{'=' * 70} ")

            # === ä¿å­˜è®­ç»ƒç»“æœ ===
    pd.DataFrame({'Ro': ro_history}).to_csv(os.path.join(output_dir, 'ro_history.csv'), index=False)
    torch.save(model.state_dict(), os.path.join(output_dir, 'model_final.pt'))
    print(f"âœ… æ¨¡å‹è®­ç»ƒå®Œæˆï¼ŒRo æ¼”åŒ–ä¸å‚æ•°å·²ä¿å­˜åˆ°: {output_dir}")

    summary = [
                  "âœ… æ¨¡å‹è®­ç»ƒå‚æ•°è®°å½•",
                  f"Ro åˆå§‹å€¼: {config.Ro}",
                  f"Ro æœ€ç»ˆå€¼: {model.Ro.item():.6f}",
                  f"omega_0: {config.omega_0}",
                  f"velocity_scale: {config.velocity_scale}",
                  f"grad_clip: {config.grad_clip}",
                  f"è®­ç»ƒè½®æ•°: {num_epochs}",
                  f"åŸºç¡€å­¦ä¹ ç‡: 1e-4",
                  f"RoåŸºå‡†å­¦ä¹ ç‡: 5e-4",
                  "æŸå¤±æƒé‡:"
              ] + [f"  {k}: {v}" for k, v in loss_manager.weights.items()] + [
                  "å­¦ä¹ ç‡ç­–ç•¥: Roå­¦ä¹ ç‡éšæ­£åˆ™æƒé‡åŒæ­¥è¡°å‡",
                  "ç‰©ç†æƒé‡ä¸Šé™: 5.0 ",
                  "Roæ­£åˆ™çº¦æŸç³»æ•°: 50 "
              ]
    theta_abs = torch.mean(torch.abs(theta)).item()
    eta_abs = torch.mean(torch.abs(eta)).item()
    summary += [
        f"theta_abs: {theta_abs:.6f}",
        f"eta_abs: {eta_abs:.6f}"
    ]

    with open(os.path.join(output_dir, "training_summary.txt"), "w") as f:
         f.write("\n".join(summary))

    # === ä½¿ç”¨æµ‹è¯•é›†åšé¢„æµ‹å¹¶ä¿å­˜ç»“æœ ===
    print("âœ… å¼€å§‹ä½¿ç”¨æµ‹è¯•é›†è¿›è¡Œé¢„æµ‹...")

    # åŠ è½½æµ‹è¯•é›†æ•°æ®
    t_test, x_test, y_test, z_test, u_true_test, v_true_test, _, original_test_df = load_csv_data_from_df(test_df,
                                                                                                          device)
    x_test.requires_grad_(True)
    y_test.requires_grad_(True)

    # æ‰§è¡Œé¢„æµ‹
    (u_pred_test, v_pred_test, u_bar, v_bar, theta, eta, Z0,
     P, g1, g2, h1, h2, time_phase_C2, time_phase_C3) = compute_velocity(model, x_test, y_test, z_test, t_test)

    # è®¡ç®—æŸå¤±é¡¹
    final_losses = {
        'data': value_loss(u_pred_test, v_pred_test, u_true_test, v_true_test),
        'dir': direction_loss(u_pred_test, v_pred_test, u_true_test, v_true_test),
        'phys': torch.mean(compute_residuals(model, x_test, y_test, z_test, t_test)[0] ** 2 +
                           compute_residuals(model, x_test, y_test, z_test, t_test)[1] ** 2),
        'cont': torch.mean(compute_residuals(model, x_test, y_test, z_test, t_test)[2] ** 2),
        'geo': geometric_constraint(g1, g2, Z0, x_test, y_test),
        'act': activation_loss(theta, eta)
    }

    # âœ… åœ¨è¿™é‡Œæ·»åŠ ï¼šè®¡ç®—æ­£ç¡®çš„æ–¹å‘è§’ MAEï¼ˆå¸¦æ©ç ï¼Œä½¿ç”¨å¼§åº¦ï¼‰
    true_mag = torch.sqrt(u_true_test ** 2 + v_true_test ** 2)
    mask = (true_mag > 0.005)  # ä¸è®­ç»ƒæ—¶ç›¸åŒçš„æ©ç 

    pred_angle = torch.atan2(v_pred_test, u_pred_test)
    true_angle = torch.atan2(v_true_test, u_true_test)  # âœ… ä¿®æ­£ï¼šu_true_test

    # è®¡ç®—è§’åº¦å·®ï¼ˆå¼§åº¦ï¼‰
    angle_diff = torch.abs(pred_angle - true_angle)

    # å¤„ç†è§’åº¦å·®è¶…è¿‡ Ï€ çš„æƒ…å†µï¼ˆæ–¹å‘ç›¸åï¼‰
    angle_diff = torch.min(angle_diff, 2 * torch.pi - angle_diff)

    # ä»…åœ¨é€Ÿåº¦å¤§çš„åŒºåŸŸè®¡ç®— MAEï¼ˆå¼§åº¦ï¼‰
    direction_mae_rad = angle_diff[mask].mean().item()

    # å°†æ­£ç¡®çš„ MAE æ·»åŠ åˆ° final_losses ä¸­ï¼ˆç”¨äºä¿å­˜ï¼‰
    final_losses['direction_mae'] = torch.tensor(direction_mae_rad)

    saver = PredictionSaver(model, scaler_mgr, output_dir)
    saver.save_all(
        original_df=original_test_df,
        u_pred=u_pred_test, v_pred=v_pred_test,
        u_true=u_true_test, v_true=v_true_test,
        Z0=Z0, x=x_test, y=y_test, z=z_test, t=t_test,
        g1=g1, g2=g2,
        final_losses=final_losses,
        extra_metrics={'Ro': model.Ro.item()}
    )

    print("âœ… æµ‹è¯•é›†é¢„æµ‹å®Œæˆï¼Œç»“æœå·²ä¿å­˜åˆ°:", output_dir)

    print("ğŸ”„ Start drawing comparison plots...")

    import matplotlib.pyplot as plt
    import numpy as np

    # Convert to numpy arrays for plotting
    u_pred_np = u_pred_test.detach().cpu().numpy().flatten()
    v_pred_np = v_pred_test.detach().cpu().numpy().flatten()
    u_true_np = u_true_test.detach().cpu().numpy().flatten()
    v_true_np = v_true_test.detach().cpu().numpy().flatten()

    indices = np.arange(len(u_pred_np))  # ä½¿ç”¨å…¨éƒ¨æµ‹è¯•ç‚¹

    # Create three separate figures
    fig, axes = plt.subplots(3, 1, figsize=(15, 15))

    # Plot 1: u_pred vs u_true
    axes[0].plot(indices, u_true_np[indices], 'b-', linewidth=1.5, alpha=0.8, label='u_true')
    axes[0].plot(indices, u_pred_np[indices], 'r-', linewidth=1.5, alpha=0.8, label='u_pred')
    axes[0].set_xlabel('Data Point Index')
    axes[0].set_ylabel('u Velocity')
    axes[0].set_title(f'u_pred vs u_true Comparison (All test Points)')
    axes[0].legend()
    axes[0].grid(True, alpha=0.3)

    # Plot 2: v_pred vs v_true
    axes[1].plot(indices, v_true_np[indices], 'g-', linewidth=1.5, alpha=0.8, label='v_true')
    axes[1].plot(indices, v_pred_np[indices], 'm-', linewidth=1.5, alpha=0.8, label='v_pred')
    axes[1].set_xlabel('Data Point Index')
    axes[1].set_ylabel('v Velocity')
    axes[1].set_title(f'v_pred vs v_true Comparison (All test Points)')
    axes[1].legend()
    axes[1].grid(True, alpha=0.3)

    # Calculate true direction angles
    true_magnitude = np.sqrt(u_true_np ** 2 + v_true_np ** 2)
    true_u_norm = u_true_np / (true_magnitude + 1e-8)  # Avoid division by zero
    true_v_norm = v_true_np / (true_magnitude + 1e-8)
    true_angle = np.arctan2(true_v_norm, true_u_norm) * 180 / np.pi  # Convert to degrees

    # Calculate predicted direction angles
    pred_magnitude = np.sqrt(u_pred_np ** 2 + v_pred_np ** 2)
    pred_u_norm = u_pred_np / (pred_magnitude + 1e-8)
    pred_v_norm = v_pred_np / (pred_magnitude + 1e-8)
    pred_angle = np.arctan2(pred_v_norm, pred_u_norm) * 180 / np.pi

    axes[2].plot(indices, true_angle[indices], 'c-', linewidth=1.5, alpha=0.8, label='True Direction Angle')
    axes[2].plot(indices, pred_angle[indices], 'y-', linewidth=1.5, alpha=0.8, label='Predicted Direction Angle')
    axes[2].set_xlabel('Data Point Index')
    axes[2].set_ylabel('Direction Angle (degrees)')
    axes[2].set_title(f'Velocity Direction Angle Comparison (All test Points)')
    axes[2].legend()
    axes[2].grid(True, alpha=0.3)

    plt.tight_layout()

    # Save the plot
    comparison_plot_path = os.path.join(output_dir, 'velocity_comparison_plots.png')
    plt.savefig(comparison_plot_path, dpi=300, bbox_inches='tight')
    plt.close()

    # Save comparison data for the selected points
    # --- åŸæœ‰ä»£ç ï¼šåˆ°ä¿å­˜ comparison_data ä¸ºæ­¢ï¼Œä¿æŒä¸å˜ ---

    comparison_data = pd.DataFrame({
        'index': indices,
        'u_true': u_true_np[indices],
        'u_pred': u_pred_np[indices],
        'v_true': v_true_np[indices],
        'v_pred': v_pred_np[indices],
        'true_angle_deg': true_angle[indices],
        'pred_angle_deg': pred_angle[indices],
        'angle_difference_deg': np.abs(true_angle[indices] - pred_angle[indices])
    })

    comparison_csv_path = os.path.join(output_dir, 'velocity_comparison_data.csv')
    comparison_data.to_csv(comparison_csv_path, index=False)

    # æ·»åŠ æ©ç ï¼Œåªåœ¨é€Ÿåº¦å¤§çš„åŒºåŸŸè®¡ç®—; æå–ç”¨äºç»˜å›¾çš„å­é›†æ•°æ®
    u_true_plot = u_true_np[indices]
    v_true_plot = v_true_np[indices]
    u_pred_plot = u_pred_np[indices]
    v_pred_plot = v_pred_np[indices]

    # è®¡ç®—çœŸå®é€Ÿåº¦å¤§å°
    true_mag_plot = np.sqrt(u_true_plot ** 2 + v_true_plot ** 2)
    mask_plot = (true_mag_plot > 0.005)  # ä¸è®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´çš„æ©ç 

    # è®¡ç®— MAEï¼ˆä»…åœ¨æ©ç åŒºåŸŸå†…ï¼‰
    u_mae = np.mean(np.abs(u_pred_plot[mask_plot] - u_true_plot[mask_plot]))
    v_mae = np.mean(np.abs(v_pred_plot[mask_plot] - v_true_plot[mask_plot]))

    # å¤„ç†è§’åº¦å·®ï¼ˆé¿å… 350Â° å’Œ 10Â° çš„å·®ä¸º 340Â°ï¼‰
    angle_diff = np.abs(true_angle[indices] - pred_angle[indices])
    angle_diff = np.minimum(angle_diff, 360 - angle_diff)  # æ­£ç¡®å¤„ç†è§’åº¦ç¯ç»•
    angle_mae = np.mean(angle_diff[mask_plot])

    # --- åŸæœ‰ç»˜å›¾ä»£ç ä¸å˜ï¼Œä»…ç»Ÿè®¡è¾“å‡ºä½¿ç”¨ mask ---
    print(f"\nğŸ“Š Prediction Accuracy Statistics (for all {len(u_pred_np)} test points):")
    print(f"  u-component MAE: {u_mae:.6f}")
    print(f"  v-component MAE: {v_mae:.6f}")
    print(f"  Direction Angle MAE: {angle_mae:.2f}Â°")
    print(f"  Comparison data saved to: {comparison_csv_path}")
    print(f"  Comparison plots saved to: {comparison_plot_path}")

if __name__ == "__main__":
    args = parse_args_for_train()
    # è‹¥å¸Œæœ›ç¡®ä¿ output è·¯å¾„å§‹ç»ˆåœ¨ä»“åº“è„šæœ¬ç›®å½•ä¸‹ï¼ˆå³ä¸å—å½“å‰å·¥ä½œç›®å½•å½±å“ï¼‰ï¼Œå¯ä»¥æŠŠä¸‹é¢ä¸€è¡Œæ›¿æ¢ä¸ºï¼š
    # repo_root = os.path.dirname(os.path.abspath(__file__))
    # output_dir = os.path.join(repo_root, args.output)
    output_dir = args.output
    input_path = args.input
    train_prediction_model(input_path=input_path, output_dir=output_dir, device=args.device)